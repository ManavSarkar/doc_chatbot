{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Set logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just an AI, I don't have feelings like humans do, but I'm functioning properly and ready to help you with any questions or tasks you have! It's great to chat with you. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 16, 'total_tokens': 65, 'completion_time': 0.040833333, 'prompt_time': 0.002372446, 'queue_time': 0.0007843940000000003, 'total_time': 0.043205779}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-473ee1a7-132d-443d-a7f5-36ae87ff46f4-0', usage_metadata={'input_tokens': 16, 'output_tokens': 49, 'total_tokens': 65})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"MISTRALAI_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "model.invoke(\"Hello, how are you!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    return PyMuPDFLoader(file_path).load()\n",
    "\n",
    "documents = load_pdf(\"input/gen_ai_langchain_2024.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 361 and Number of Chunks: 1014\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "chunks = split_documents(documents)\n",
    "\n",
    "# add unique ids to each chunk combining source, page and start_index\n",
    "for chunk in chunks:\n",
    "    source = chunk.metadata.get(\"source\")\n",
    "    page = chunk.metadata.get(\"page\")\n",
    "    start_index = chunk.metadata.get(\"start_index\")\n",
    "    chunk.metadata[\"id\"] = f\"{source}_{page}_{start_index}\"\n",
    "\n",
    "print(f\"Number of Documents: {len(documents)} and Number of Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunks[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# test the embedding function\n",
    "embedding_function = get_embedding_function()\n",
    "len(embedding_function.embed_query(\"Hello, how are you!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\temp\\ipykernel_9492\\3526497029.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing items: 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=get_embedding_function(),\n",
    "    )\n",
    "    \n",
    "    # add or update the chunks in the chroma db\n",
    "    existing_items = db.get(include=[])\n",
    "    if existing_items is None or \"id\" not in existing_items:\n",
    "        existing_items = {\"id\": []}\n",
    "    existing_ids = set(existing_items[\"id\"])\n",
    "    \n",
    "    print(f\"Number of existing items: {len(existing_ids)}\")\n",
    "    \n",
    "    new_chunks = [chunk for chunk in chunks if chunk.metadata[\"id\"] not in existing_ids]\n",
    "    new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "    db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "    db.persist()\n",
    "    \n",
    "try:\n",
    "    add_to_chroma(chunks)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based on only on the following context:\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Answer the question based on the context above: \n",
    "```\n",
    "{question}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "def query_rag(query_text: str, model):\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=get_embedding_function(),\n",
    "    )\n",
    "    \n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "    \n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    \n",
    "    response = model.invoke(prompt)\n",
    "    \n",
    "    sources = [doc.metadata.get(\"id\") for doc, _score in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
